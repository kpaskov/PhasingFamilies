{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Array-CGH is the gold standard CNV detection method. This paper: \n",
    "\n",
    "Celestino-Soper PB, Shaw CA, Sanders SJ, Li J, Murtha MT, Ercan-Sencicek AG, Davis L, Thomson S, Gambin T, Chinault AC, Ou Z. Use of array CGH to detect exonic copy number variants throughout the genome in autism families detects a novel deletion in TMLHE. Human molecular genetics. 2011 Aug 24;20(22):4360-70.\n",
    "\n",
    "ran aCGH on 99 probands from SSC. We ran our deletion detection algorithm on the same individuals (from quads) using WGSd data. We want to know how the deletions we detected compare to the deletions detected by aCGH.\n",
    "\n",
    "As a first pass, we'll look for the deletions they confirmed (so we're evaluating sensitivity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Deletion = namedtuple('Deletion', ['family', 'individual', 'chrom', 'start_pos', 'end_pos', 'length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num DDR dels 167\n"
     ]
    }
   ],
   "source": [
    "ddr_dels = []\n",
    "with open('../other_sv_calls/ddr363_Supplementary_Data/valid_dels.txt', 'r') as f:\n",
    "    next(f) # skip header\n",
    "    for line in f:\n",
    "        pieces = line.strip().split('\\t')\n",
    "        if pieces[10].lower() == 'loss':\n",
    "            fam_id, chrom, start, end  = pieces[0], pieces[2][3:], int(pieces[7]), int(pieces[8])\n",
    "            ddr_dels.append(Deletion(fam_id, fam_id +  '.p1', chrom, start, end, end-start+1))\n",
    "print('Num DDR dels', len(ddr_dels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 families of size 3\n",
      "516 families of size 4\n"
     ]
    }
   ],
   "source": [
    "# pull families that we've phased\n",
    "phase_dir = '../sherlock_phased_ssc'\n",
    "\n",
    "family_to_individuals = dict()\n",
    "for j in [3, 4]:\n",
    "    with open('%s/chr.%s.familysize.%d.families.txt' % (phase_dir, chrom, j), 'r')  as f:\n",
    "        next(f) # skip header\n",
    "        num_fams_of_size = 0\n",
    "        for line in f:\n",
    "            pieces = line.strip().split('\\t')\n",
    "            family_to_individuals[pieces[0].split('.')[0]] = pieces[1:(1+j)]\n",
    "            num_fams_of_size += 1\n",
    "        print('%d families of size %d' % (num_fams_of_size, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Families 12 Individuals 47\n"
     ]
    }
   ],
   "source": [
    "# find intersection between families\n",
    "phased_families = set([x.split('.')[0] for x in family_to_individuals.keys()])\n",
    "ddr_families = set([x.family for x in ddr_dels])\n",
    "intersect_families = phased_families & ddr_families\n",
    "\n",
    "individuals = sum([family_to_individuals[x] for x in intersect_families], [])\n",
    "sample_id_to_index = dict([(x, i) for i, x in enumerate(individuals)])\n",
    "m = len(individuals)\n",
    "print('Families', len(intersect_families), 'Individuals', m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n 18137\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "chrom = '22'\n",
    "\n",
    "# pull snp_positions for this chromosome\n",
    "DATA = np.load('%s/chr.%s.deletions.npz' % (phase_dir, chrom))\n",
    "snp_positions = DATA['snp_positions']\n",
    "\n",
    "# add snp_positions from ddr deletions\n",
    "snp_positions = set(snp_positions.tolist())\n",
    "snp_positions = snp_positions | set(sum([[x.start_pos, x.end_pos] for x in ddr_dels \\\n",
    "                                         if x.family in intersect_families and x.chrom == chrom], []))\n",
    "snp_positions = np.asarray(sorted(snp_positions))\n",
    "pos_to_index = dict([(x, i) for i, x in enumerate(snp_positions)])\n",
    "n = snp_positions.shape[0]\n",
    "print('n', n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load ddr deletions\n",
    "ddr_deletions = np.zeros((m, n))\n",
    "for d in ddr_dels:\n",
    "    start_index, end_index = pos_to_index[d.start_pos], pos_to_index[d.end_pos]\n",
    "    ddr_deletions[sample_id_to_index[d.individual], start_index:end_index] = 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
