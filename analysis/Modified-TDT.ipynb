{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter, namedtuple\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import scipy.stats\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroms = [str(x) for x in range(1, 23)]\n",
    "#chroms = ['18']\n",
    "ihart_family_sizes = [3, 4, 5, 6]\n",
    "ihart_phase_dir = '../phased_ihart'\n",
    "ssc_family_sizes = [3, 4]\n",
    "ssc_phase_dir = '../phased_ssc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ped_files = ['../data/160826.ped', '../data/ssc.ped']\n",
    "# Affection (0=unknown; 1=unaffected; 2=affected)\n",
    "child_id_to_affected = dict()\n",
    "child_id_to_sex = dict()\n",
    "fam_to_inds = dict()\n",
    "\n",
    "for ped_file in ped_files:\n",
    "    with open(ped_file, 'r') as f:\n",
    "        for line in f:\n",
    "            pieces = line.strip().split('\\t')\n",
    "            if len(pieces) >= 6:\n",
    "                fam_id, child_id, f_id, m_id, sex, disease_status = pieces[0:6]\n",
    "                famkey = '.'.join((fam_id, m_id, f_id))\n",
    "                if famkey not in fam_to_inds:\n",
    "                    fam_to_inds[famkey] = [m_id, f_id]\n",
    "                fam_to_inds[famkey].append(child_id)\n",
    "                child_id_to_affected[child_id] = disease_status\n",
    "                child_id_to_sex[child_id] = sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 249250621, 492449994, 690472424, 881626700, 1062541960, 1233657027, 1392795690, 1539159712, 1680373143, 1815907890, 1950914406, 2084766301, 2199936179, 2307285719, 2409817111, 2500171864, 2581367074, 2659444322, 2718573305, 2781598825, 2829728720, 2881033286]\n"
     ]
    }
   ],
   "source": [
    "# From GRCh37.p13 https://www.ncbi.nlm.nih.gov/grc/human/data?asm=GRCh37.p13\n",
    "chrom_lengths = {\n",
    "\t'1': 249250621,\n",
    "\t'2': 243199373,\n",
    "\t'3': 198022430,\n",
    "\t'4': 191154276,\n",
    "\t'5': 180915260,\n",
    "\t'6': 171115067,\n",
    "\t'7': 159138663,\n",
    "\t'8': 146364022,\n",
    "\t'9': 141213431,\n",
    "\t'10': 135534747,\n",
    "\t'11': 135006516,\n",
    "\t'12': 133851895,\n",
    "\t'13': 115169878,\n",
    "\t'14': 107349540,\n",
    "\t'15': 102531392,\n",
    "\t'16': 90354753,\n",
    "\t'17': 81195210,\n",
    "\t'18': 78077248,\n",
    "\t'19': 59128983,\n",
    "\t'20': 63025520,\n",
    "\t'21': 48129895,\n",
    "\t'22': 51304566,\n",
    "\t'X': 155270560,\n",
    "\t'Y': 59373566\n",
    "}\n",
    "\n",
    "chrom_offsets = [0]\n",
    "for chrom in chroms:\n",
    "    chrom_offsets.append(chrom_offsets[-1]+chrom_lengths[chrom])\n",
    "print(chrom_offsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Families\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 Families with all chroms 871\n",
      "Counter({22: 871})\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 Families with all chroms 519\n",
      "Counter({22: 519})\n"
     ]
    }
   ],
   "source": [
    "family_to_individuals = dict()\n",
    "\n",
    "def load_families(phase_dir, family_sizes):\n",
    "    family_to_chroms = defaultdict(set)\n",
    "    \n",
    "    for chrom in chroms:\n",
    "        print(chrom, end=' ')\n",
    "\n",
    "        for j in family_sizes:\n",
    "            try:\n",
    "                with open('%s/chr.%s.familysize.%d.families.txt' % (phase_dir, chrom, j), 'r') as f:\n",
    "                    next(f) # skip header\n",
    "                    for line in f:\n",
    "                        pieces = line.strip().split('\\t')\n",
    "                        family_key = pieces[0]\n",
    "                        family_to_chroms[family_key].add(chrom)\n",
    "                        family_to_individuals[family_key] = pieces[1:(1+j)]\n",
    "            except FileNotFoundError:\n",
    "                print('File not found', 'chrom', chrom, 'family size', j)\n",
    "            except StopIteration:\n",
    "                print('File empty', 'chrom', chrom, 'family size', j)\n",
    "\n",
    "    families_to_include = set([k for k, v in family_to_chroms.items() if len(v)==len(chroms)])\n",
    "    print('Families with all chroms', len(families_to_include))\n",
    "    print(Counter([len(v) for v in family_to_chroms.values()]))\n",
    "    return families_to_include\n",
    "ihart_families = load_families(ihart_phase_dir, ihart_family_sizes)\n",
    "ssc_families = load_families(ssc_phase_dir, ssc_family_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ihart_individuals = sorted(sum([family_to_individuals[k] for k in ihart_families], []))\n",
    "ihart_ind_to_index = dict([(x, i) for i, x in enumerate(ihart_individuals)])\n",
    "ssc_individuals = sorted(sum([family_to_individuals[k] for k in ssc_families], []))\n",
    "ssc_ind_to_index = dict([(x, i) for i, x in enumerate(ssc_individuals)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in deletions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Deletion = namedtuple('Deletion', ['family', 'chrom', 'start_pos', 'end_pos', 'length', 'opt_start_pos', 'opt_end_pos', 'trans', 'notrans', \n",
    "                                      'family_size', 'is_mat', 'is_pat', 'state', 'dataset', 'supporting_sites'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def read_deletions(chrom, phase_dir, family_sizes, families, dataset):\n",
    "    deletions = []\n",
    "\n",
    "    for j in family_sizes:\n",
    "            \n",
    "        # load deletions\n",
    "        family_to_states = defaultdict(list)\n",
    "        family_to_pos = defaultdict(list)\n",
    "        family_to_indices = defaultdict(list)\n",
    "        with open('%s/chr.%s.familysize.%d.phased.txt' % (phase_dir, chrom, j), 'r')  as f:\n",
    "            next(f) # skip header\n",
    "\n",
    "            for line in f:\n",
    "                pieces = line.strip().split('\\t')\n",
    "                family_key = pieces[0]\n",
    "                if family_key in families:\n",
    "                    state = [int(x) for x in pieces[1:(2+(j*2))]]\n",
    "                    start_pos, end_pos = [int(x) for x in pieces[(2+(j*2)):(4+(j*2))]]\n",
    "                    start_index, end_index = [int(x) for x in pieces[(4+(j*2)):(6+(j*2))]]\n",
    "                    assert end_pos >= start_pos\n",
    "                        \n",
    "                    # if a segment is never inherited, we don't know if it has a deletion or not\n",
    "                    if state[0] == 0 and state[1] == 0:\n",
    "                        pass\n",
    "                    else:\n",
    "                        if len([i for i in range(4, len(state)-1, 2) if state[i]==0]) == 0:\n",
    "                            state[0] = -1\n",
    "                        if len([i for i in range(4, len(state)-1, 2) if state[i]==1]) == 0:\n",
    "                            state[1] = -1\n",
    "                    if state[2] == 0 and state[3] == 0:\n",
    "                        pass\n",
    "                    else:\n",
    "                        if len([i for i in range(5, len(state)-1, 2) if state[i]==0]) == 0:\n",
    "                            state[2] = -1\n",
    "                        if len([i for i in range(5, len(state)-1, 2) if state[i]==1]) == 0:\n",
    "                            state[3] = -1\n",
    "\n",
    "                    family_to_states[family_key].append(state)\n",
    "                    family_to_pos[family_key].append((start_pos, end_pos))\n",
    "                    family_to_indices[family_key].append((start_index, end_index))\n",
    "\n",
    "        # for each family, detect deletion transmission\n",
    "        for family_key, states in family_to_states.items():\n",
    "            states = np.asarray(states)\n",
    "            assert np.all(states[0, :4] != 0)\n",
    "            assert np.all(states[-1, :4] != 0)\n",
    "            positions = np.asarray(family_to_pos[family_key])\n",
    "            pos_indices = np.asarray(family_to_indices[family_key])\n",
    "            inds = family_to_individuals[family_key]\n",
    "\n",
    "            # for each ancestral chromosome\n",
    "            for anc in range(4):\n",
    "                is_mat = anc==0 or anc==1\n",
    "                is_pat = anc==2 or anc==3\n",
    "                    \n",
    "                start_indices = np.where((states[:-1, anc] != 0) & (states[1:, anc] == 0))[0]+1\n",
    "                end_indices = np.where((states[:-1, anc] == 0) & (states[1:, anc] != 0))[0]+1\n",
    "                for s_ind, e_ind in zip(start_indices, end_indices):\n",
    "                    \n",
    "                    # check if parental double deletion\n",
    "                    if is_mat:\n",
    "                        is_double = np.all(states[s_ind, :2]==0)\n",
    "                    else:\n",
    "                        is_double = np.all(states[s_ind, 2:4]==0)\n",
    "        \n",
    "                    # check if recombination event occured and that inheritance state is known\n",
    "                    has_recomb = False\n",
    "                    if is_mat:\n",
    "                        indices = np.arange(4, states.shape[1]-1, 2)\n",
    "                    else:\n",
    "                        indices = np.arange(5, states.shape[1]-1, 2)\n",
    "                        \n",
    "                    inh_known = np.all(states[s_ind:e_ind, indices] != -1)\n",
    "                        \n",
    "                    for i in range(s_ind, e_ind):\n",
    "                        if np.any(states[i, indices] != states[s_ind, indices]):\n",
    "                            has_recomb = True\n",
    "                            \n",
    "                    # check if in ok region\n",
    "                    ok_region = np.all(states[s_ind:e_ind, -1] == 0)\n",
    "                    \n",
    "                    if ok_region and inh_known and (not has_recomb):\n",
    "                        start_pos, end_pos = positions[s_ind, 0], positions[e_ind-1, 1]\n",
    "                        start_pos_index, end_pos_index = pos_indices[s_ind, 0], pos_indices[e_ind-1, 1]\n",
    "                        \n",
    "                        # find boundaries of the deletion\n",
    "                        #print(s_ind, e_ind)\n",
    "                        opt_start_index = s_ind\n",
    "                        while states[opt_start_index, anc] != 1 and opt_start_index > 0:\n",
    "                            opt_start_index -= 1\n",
    "                        opt_start_pos = positions[opt_start_index+1, 0]\n",
    "                        \n",
    "                        opt_end_index = e_ind\n",
    "                        while(states[opt_end_index, anc]) != 1 and opt_end_index < states.shape[0]-1:\n",
    "                            opt_end_index += 1\n",
    "                        opt_end_pos = positions[e_ind-1, 1]\n",
    "                        \n",
    "                        assert start_pos <= end_pos\n",
    "                        assert opt_start_pos <= start_pos\n",
    "                        assert end_pos <= opt_end_pos\n",
    "                        \n",
    "                        state = states[s_ind, :]\n",
    "                    \n",
    "                        # children\n",
    "                        trans, notrans = [], []\n",
    "                        for k, child in zip(range(2, j), inds[2:]):\n",
    "                            mom_s, dad_s = state[(2*k):(2*(k+1))]\n",
    "\n",
    "                            if is_mat:\n",
    "                                assert mom_s != -1\n",
    "                                if anc==mom_s:\n",
    "                                    trans.append(child)\n",
    "                                else:\n",
    "                                    notrans.append(child)\n",
    "                            if is_pat:\n",
    "                                assert dad_s != -1\n",
    "                                if anc==2+dad_s:\n",
    "                                    trans.append(child)\n",
    "                                else:\n",
    "                                    notrans.append(child)\n",
    "\n",
    "                        if (len(trans) + len(notrans) == j-2) and (len(trans) > 0):\n",
    "                            deletions.append(Deletion(family_key, chrom, start_pos, end_pos, end_pos-start_pos+1,\n",
    "                                                      opt_start_pos, opt_end_pos,\n",
    "                                                              tuple(trans), tuple(notrans), j, is_mat, is_pat,\n",
    "                                                              tuple(states[s_ind, :]), dataset,\n",
    "                                                    end_pos_index-start_pos_index+1))\n",
    "                        \n",
    "    # sort deletions\n",
    "    deletions = sorted(deletions, key=lambda x: x.start_pos)\n",
    "    return deletions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeletionCollection:\n",
    "    def __init__(self, deletion, matches):\n",
    "        self.deletion = deletion\n",
    "        self.matches = matches\n",
    "\n",
    "def create_collections(deletions):\n",
    "    collections = []\n",
    "    \n",
    "    starts = np.array([d.start_pos for d in deletions])\n",
    "    stops = np.array([d.end_pos for d in deletions])\n",
    "\n",
    "    ordered_start_indices = np.argsort(starts)\n",
    "    ordered_starts = starts[ordered_start_indices]\n",
    "    ordered_stop_indices = np.argsort(stops)\n",
    "    ordered_stops = stops[ordered_stop_indices]\n",
    "        \n",
    "    insert_starts_in_stops = np.searchsorted(ordered_stops, starts)\n",
    "    insert_stops_in_starts = np.searchsorted(ordered_starts, stops, side='right')\n",
    "        \n",
    "    indices = np.ones((len(deletions),), dtype=bool)\n",
    "\n",
    "    for del_index, main_d in enumerate(deletions):\n",
    "        indices[:] = True\n",
    "        indices[ordered_stop_indices[:insert_starts_in_stops[del_index]]] = False\n",
    "        indices[ordered_start_indices[insert_stops_in_starts[del_index]:]] = False\n",
    "\n",
    "        collections.append(DeletionCollection(main_d, [deletions[j] for j in np.where(indices)[0]]))\n",
    "    return collections\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_deletions(collections, share_cutoff=0.8):    \n",
    "    # first, prune deletions that don't overlap with the main deletion by share_cutoff\n",
    "    for c in collections:\n",
    "        lengths = np.array([d.length for d in c.matches])\n",
    "        overlaps1 = np.array([min(d.opt_end_pos, c.deletion.end_pos)-max(d.opt_start_pos, c.deletion.start_pos)+1 for d in c.matches])\n",
    "        overlaps2 = np.array([min(d.end_pos, c.deletion.opt_end_pos)-max(d.start_pos, c.deletion.opt_start_pos)+1 for d in c.matches])\n",
    "        c.matches = set([c.matches[j] for j in np.where((overlaps2 >= 0) & (overlaps1 >= share_cutoff*c.deletion.length))[0]])\n",
    "    return collections\n",
    "\n",
    "def prune_collections(collections):\n",
    "        \n",
    "    deletion_to_index = dict([(x.deletion, i) for i, x in enumerate(collections)])\n",
    "    \n",
    "    # now, get rid of collections that are identical to other collections\n",
    "    for c in collections:\n",
    "        if c is not None:\n",
    "            for d in c.matches:\n",
    "                index = deletion_to_index[d]\n",
    "                if (c.deletion != d) and (collections[index] is not None) and (c.matches == collections[index].matches):\n",
    "                    collections[index] = None\n",
    "    #print('removing %d of %d' % (len([x for x in collections if x is None]), len(collections)))\n",
    "    collections = [x for x in collections if x is not None]\n",
    "    return collections\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 id sd c p 3474\n",
      "2 id sd c p 2599\n",
      "3 id sd c p 2423\n",
      "4 id sd c p 3055\n",
      "5 id sd c p 1948\n",
      "6 id sd c p 3861\n",
      "7 id sd c p 2572\n",
      "8 id sd c p 2602\n",
      "9 id sd c p 2202\n",
      "10 id sd c p 1588\n",
      "11 id sd c p 2360\n",
      "12 id sd c p 2105\n",
      "13 id sd c p 1129\n",
      "14 id sd c p 2112\n",
      "15 id sd c p 1674\n",
      "16 id sd c p 1230\n",
      "17 id sd c p 1021\n",
      "18 id sd c p 855\n",
      "19 id sd c p 1921\n",
      "20 id sd c p 653\n",
      "21 id sd c p 549\n",
      "22 id sd c p 819\n"
     ]
    }
   ],
   "source": [
    "all_collections = []\n",
    "for chrom in chroms:\n",
    "    print(chrom, end=' ')\n",
    "    \n",
    "    ihart_deletions = read_deletions(chrom, ihart_phase_dir, ihart_family_sizes, ihart_families, 'iHART')\n",
    "    print('id', end=' ')\n",
    "    \n",
    "    ssc_deletions = read_deletions(chrom, ssc_phase_dir, ssc_family_sizes, ssc_families, 'SSC')\n",
    "    print('sd', end=' ')\n",
    "    \n",
    "    deletions = sorted(ihart_deletions + ssc_deletions, key=lambda x: x.start_pos)\n",
    "    collections = create_collections(deletions)\n",
    "    print('c', end=' ')\n",
    "    \n",
    "    collections = prune_deletions(collections)\n",
    "    print('p', end=' ')\n",
    "    \n",
    "    collections = prune_collections(collections)\n",
    "    print(len(collections))\n",
    "    \n",
    "    all_collections.append(collections)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified TDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contingency(collections, family_sizes, filter_child, verbose=True):\n",
    "    # chrom, collection, family_size, notrans/trans\n",
    "    contingency = [np.zeros((len(collections[i]), len(family_sizes), 2)) for i in range(len(chroms))]\n",
    "    familysize_to_index = dict([(x, i) for i, x in enumerate(family_sizes)])\n",
    "    for i, chrom in enumerate(chroms):\n",
    "        if verbose:\n",
    "            print(chrom, end=' ')\n",
    "        for j, c in enumerate(collections[i]):\n",
    "            for k, s in enumerate(family_sizes):\n",
    "                contingency[i][j, k, 0] = sum([len([x for x in d.notrans if filter_child(x, d.is_mat, d.dataset)]) for d in c.matches if d.family_size==s])\n",
    "                contingency[i][j, k, 1] = sum([len([x for x in d.trans if filter_child(x, d.is_mat, d.dataset)]) for d in c.matches if d.family_size==s])\n",
    "    return contingency\n",
    "\n",
    "contingency_both_aff = create_contingency(all_collections, ihart_family_sizes, lambda child_id, is_mat, dataset: child_id_to_affected[child_id]=='2' and child_id_to_sex[child_id]=='1')\n",
    "contingency_both_unaff = create_contingency(all_collections, ihart_family_sizes, lambda child_id, is_mat, dataset: child_id_to_affected[child_id]=='1' and child_id_to_sex[child_id]=='1')\n",
    "\n",
    "contingency_ihart_aff = create_contingency(all_collections, ihart_family_sizes, lambda child_id, is_mat, dataset: child_id_to_affected[child_id]=='2' and dataset=='iHART' and child_id_to_sex[child_id]=='1')\n",
    "contingency_ihart_unaff = create_contingency(all_collections, ihart_family_sizes, lambda child_id, is_mat, dataset: child_id_to_affected[child_id]=='1' and dataset=='iHART' and child_id_to_sex[child_id]=='1')\n",
    "\n",
    "contingency_ssc_aff = create_contingency(all_collections, ihart_family_sizes, lambda child_id, is_mat, dataset: child_id_to_affected[child_id]=='2' and dataset=='SSC' and child_id_to_sex[child_id]=='1')\n",
    "contingency_ssc_unaff = create_contingency(all_collections, ihart_family_sizes, lambda child_id, is_mat, dataset: child_id_to_affected[child_id]=='1' and dataset=='SSC' and child_id_to_sex[child_id]=='1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bayesian_transmission_rate as btr\n",
    "import importlib\n",
    "importlib.reload(btr)\n",
    "\n",
    "transrates_both_aff = [btr.calculate_transmission_rates(contingency_both_aff[i], ihart_family_sizes) for i in range(len(chroms))]\n",
    "transrates_both_unaff = [btr.calculate_transmission_rates(contingency_both_unaff[i], ihart_family_sizes) for i in range(len(chroms))]\n",
    "\n",
    "transrates_ihart_aff = [btr.calculate_transmission_rates(contingency_ihart_aff[i], ihart_family_sizes) for i in range(len(chroms))]\n",
    "transrates_ihart_unaff = [btr.calculate_transmission_rates(contingency_ihart_unaff[i], ihart_family_sizes) for i in range(len(chroms))]\n",
    "              \n",
    "transrates_ssc_aff = [btr.calculate_transmission_rates(contingency_ssc_aff[i], ihart_family_sizes) for i in range(len(chroms))]\n",
    "transrates_ssc_unaff = [btr.calculate_transmission_rates(contingency_ssc_unaff[i], ihart_family_sizes) for i in range(len(chroms))]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_indices = [np.ones((len(all_collections[i]),), dtype=bool) for i in range(len(chroms))]\n",
    "for i, chrom in enumerate(chroms):\n",
    "    used_deletions = set()\n",
    "    for j in np.flip(np.argsort([sum([len(d.trans)+len(d.notrans) for d in c.matches]) for c in all_collections[i]])):\n",
    "        c = all_collections[i][j]\n",
    "        ind_indices[i][j] = np.all([x not in used_deletions for x in c.matches])\n",
    "        if ind_indices[i][j]:\n",
    "            used_deletions.update(c.matches)\n",
    "    print(np.sum(ind_indices[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "for i, chrom in enumerate(chroms):\n",
    "    \n",
    "    deletion_to_index = dict([(x.deletion, j) for j, x in enumerate(all_collections[i])])\n",
    "\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    plt.hist(transrates_ihart_aff[i][ind_indices[i]], bins=np.arange(0, 1, 0.03), alpha=0.5, label='aff', density=True)\n",
    "    plt.hist(transrates_ihart_unaff[i][ind_indices[i]], bins=np.arange(0, 1, 0.03), alpha=0.5, label='unaff', density=True)\n",
    "\n",
    "    plt.axvline(0.5, color='black', linestyle='--')\n",
    "    plt.legend()\n",
    "    plt.title(chrom)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.hstack(ind_indices).shape)\n",
    "print(np.hstack(transrates_ihart_aff).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.hstack(transrates_ihart_aff)[np.hstack(ind_indices)], np.hstack(transrates_ihart_unaff)[np.hstack(ind_indices)], alpha=0.1)\n",
    "plt.xlabel('Affected Trans Rate')\n",
    "plt.ylabel('Unaffected Trans Rate')\n",
    "plt.axhline(0.5, color='black', linestyle='--')\n",
    "plt.axvline(0.5, color='black', linestyle='--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(btr)\n",
    "\n",
    "#ihart\n",
    "posteriors_ihart_aff = [btr.calculate_posteriors(contingency_ihart_aff[i], ihart_family_sizes) for i in range(len(chroms))]\n",
    "posteriors_ihart_unaff = [btr.calculate_posteriors(contingency_ihart_unaff[i], ihart_family_sizes) for i in range(len(chroms))]\n",
    "\n",
    "#ssc\n",
    "posteriors_ssc_aff = [btr.calculate_posteriors(contingency_ssc_aff[i], ihart_family_sizes) for i in range(len(chroms))]\n",
    "posteriors_ssc_unaff = [btr.calculate_posteriors(contingency_ssc_unaff[i], ihart_family_sizes) for i in range(len(chroms))]\n",
    "\n",
    "#both\n",
    "posteriors_both_aff = [btr.calculate_posteriors(contingency_both_aff[i], ihart_family_sizes) for i in range(len(chroms))]\n",
    "posteriors_both_unaff = [btr.calculate_posteriors(contingency_both_unaff[i], ihart_family_sizes) for i in range(len(chroms))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(btr)\n",
    "\n",
    "overlap_ihart = [btr.calculate_posterior_overlap(posteriors_ihart_aff[i], posteriors_ihart_unaff[i]) for i in range(len(chroms))]\n",
    "print(np.min([np.min(overlap_ihart[i]) for i in range(len(chroms))]))\n",
    "\n",
    "overlap_ssc = [btr.calculate_posterior_overlap(posteriors_ssc_aff[i], posteriors_ssc_unaff[i]) for i in range(len(chroms))]\n",
    "print(np.min([np.min(overlap_ssc[i]) for i in range(len(chroms))]))\n",
    "\n",
    "overlap_both = [btr.calculate_posterior_overlap(posteriors_both_aff[i], posteriors_both_unaff[i]) for i in range(len(chroms))]\n",
    "print(np.min([np.min(overlap_both[i]) for i in range(len(chroms))]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    " \n",
    "cutoff = -np.log10(0.05/sum([len(contingency_both_aff[i]) for i in range(len(chroms))]))\n",
    "#cutoff = -np.log10(0.05/len(all_collections))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.axhline(cutoff, linestyle='--', color='black')\n",
    "plt.ylabel('-log10(pvalue)')\n",
    "plt.xlabel('Position')\n",
    "plt.xticks(chrom_offsets[:-1], chroms)\n",
    "plt.title('iHART')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.axhline(cutoff, linestyle='--', color='black')\n",
    "plt.ylabel('-log10(pvalue)')\n",
    "plt.xlabel('Position')\n",
    "plt.xticks(chrom_offsets[:-1], chroms)\n",
    "plt.title('SSC')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.axhline(cutoff, linestyle='--', color='black')\n",
    "plt.ylabel('-log10(pvalue)')\n",
    "plt.xlabel('Position')\n",
    "plt.xticks(chrom_offsets[:-1], chroms)\n",
    "plt.title('Both')\n",
    "\n",
    "for i, chrom in enumerate(chroms):\n",
    "    positions = chrom_offsets[i] + np.array([c.deletion.start_pos for c in all_collections[i]])\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(positions, -np.log10(overlap_ihart[i]),label='iHART', alpha=0.5)\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(positions, -np.log10(overlap_ssc[i]),label='SSC', alpha=0.5)\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(positions, -np.log10(overlap_both[i]),label='Both', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ihart\n",
    "posterior_pvalues_ihart_aff = [btr.calculate_posterior_pvalue(posteriors_ihart_aff[i]) for i in range(len(chroms))]\n",
    "posterior_pvalues_ihart_unaff =  [btr.calculate_posterior_pvalue(posteriors_ihart_unaff[i]) for i in range(len(chroms))]\n",
    "\n",
    "#ssc\n",
    "posterior_pvalues_ssc_aff =  [btr.calculate_posterior_pvalue(posteriors_ssc_aff[i]) for i in range(len(chroms))]\n",
    "posterior_pvalues_ssc_unaff =  [btr.calculate_posterior_pvalue(posteriors_ssc_unaff[i]) for i in range(len(chroms))]\n",
    "\n",
    "#both\n",
    "posterior_pvalues_both_aff =  [btr.calculate_posterior_pvalue(posteriors_both_aff[i]) for i in range(len(chroms))]\n",
    "posterior_pvalues_both_unaff =  [btr.calculate_posterior_pvalue(posteriors_both_unaff[i]) for i in range(len(chroms))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 40))\n",
    " \n",
    "cutoff = -np.log10(0.05/sum([len(contingency_both_aff[i]) for i in range(len(chroms))]))\n",
    "#cutoff = -np.log10(0.05/len(all_collections))\n",
    "\n",
    "titles = ['iHART Aff Higher', 'iHART Aff Lower', 'iHART Unaff Higher', 'iHART Unaff Lower',\n",
    "         'SSC Aff Higher', 'SSC Aff Lower', 'SSC Unaff Higher', 'SSC Unaff Lower',\n",
    "         'Both Aff Higher', 'Both Aff Lower', 'Both Unaff Higher', 'Both Unaff Lower']\n",
    "\n",
    "for i, title in enumerate(titles):\n",
    "    plt.subplot(12, 1, i+1)\n",
    "    plt.axhline(cutoff, linestyle='--', color='black')\n",
    "    plt.ylabel('-log10(pvalue)')\n",
    "    plt.xlabel('Position')\n",
    "    plt.xticks(chrom_offsets[:-1], chroms)\n",
    "    plt.title(title)\n",
    "\n",
    "for i, chrom in enumerate(chroms):\n",
    "    positions = chrom_offsets[i] + np.array([c.deletion.start_pos for c in all_collections[i]])\n",
    "    plt.subplot(12, 1, 1)\n",
    "    plt.plot(positions, -np.log10(posterior_pvalues_ihart_aff[i][:, 0]), label='aff', alpha=0.5)\n",
    "    plt.subplot(12, 1, 2)\n",
    "    plt.plot(positions, -np.log10(posterior_pvalues_ihart_aff[i][:, 1]), label='aff', alpha=0.5)\n",
    "    plt.subplot(12, 1, 3)\n",
    "    plt.plot(positions, -np.log10(posterior_pvalues_ihart_unaff[i][:, 0]), label='unaff', alpha=0.5)\n",
    "    plt.subplot(12, 1, 4)\n",
    "    plt.plot(positions, -np.log10(posterior_pvalues_ihart_unaff[i][:, 1]), label='unaff', alpha=0.5)\n",
    "    plt.subplot(12, 1, 5)\n",
    "    plt.plot(positions, -np.log10(posterior_pvalues_ssc_aff[i][:, 0]), label='aff', alpha=0.5)\n",
    "    plt.subplot(12, 1, 6)\n",
    "    plt.plot(positions, -np.log10(posterior_pvalues_ssc_aff[i][:, 1]), label='aff', alpha=0.5)\n",
    "    plt.subplot(12, 1, 7)\n",
    "    plt.plot(positions, -np.log10(posterior_pvalues_ssc_unaff[i][:, 0]), label='unaff', alpha=0.5)\n",
    "    plt.subplot(12, 1, 8)\n",
    "    plt.plot(positions, -np.log10(posterior_pvalues_ssc_unaff[i][:, 1]), label='unaff', alpha=0.5)\n",
    "    plt.subplot(12, 1, 9)\n",
    "    plt.plot(positions, -np.log10(posterior_pvalues_both_aff[i][:, 0]), label='aff', alpha=0.5)\n",
    "    plt.subplot(12, 1, 10)\n",
    "    plt.plot(positions, -np.log10(posterior_pvalues_both_aff[i][:, 1]), label='aff', alpha=0.5)\n",
    "    plt.subplot(12, 1, 11)\n",
    "    plt.plot(positions, -np.log10(posterior_pvalues_both_unaff[i][:, 0]), label='unaff', alpha=0.5)\n",
    "    plt.subplot(12, 1, 12)\n",
    "    plt.plot(positions, -np.log10(posterior_pvalues_both_unaff[i][:, 1]), label='unaff', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.hstack(transrates_both_aff)[np.hstack(ind_indices)], np.hstack(transrates_both_unaff)[np.hstack(ind_indices)], alpha=0.1)\n",
    "\n",
    "cutoff = 3\n",
    "indices = -np.log10(np.vstack(posterior_pvalues_both_aff))[:, 0]>=cutoff\n",
    "plt.scatter(np.hstack(transrates_both_aff)[indices], np.hstack(transrates_both_unaff)[indices])\n",
    "indices = -np.log10(np.vstack(posterior_pvalues_both_unaff))[:, 0]>=cutoff\n",
    "plt.scatter(np.hstack(transrates_both_aff)[indices], np.hstack(transrates_both_unaff)[indices])\n",
    "indices = -np.log10(np.vstack(posterior_pvalues_both_aff))[:, 1]>=cutoff\n",
    "plt.scatter(np.hstack(transrates_both_aff)[indices], np.hstack(transrates_both_unaff)[indices])\n",
    "indices = -np.log10(np.vstack(posterior_pvalues_both_unaff))[:, 1]>=cutoff\n",
    "plt.scatter(np.hstack(transrates_both_aff)[indices], np.hstack(transrates_both_unaff)[indices])\n",
    "\n",
    "plt.xlabel('Affected Trans Rate')\n",
    "plt.ylabel('Unaffected Trans Rate')\n",
    "plt.axhline(0.5, color='black', linestyle='--')\n",
    "plt.axvline(0.5, color='black', linestyle='--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chrom in enumerate(chroms):\n",
    "    print(chrom)\n",
    "    for j in np.where(-np.log10(posterior_pvalues_ihart_aff[i][:, 0]) > cutoff)[0]:\n",
    "        d = all_collections[i][j].deletion\n",
    "        print('%s%s:%d-%d %d %d %0.2f' % ('*' if ind_indices[i][j] else '', \n",
    "                                       d.chrom, d.opt_start_pos, d.opt_end_pos, d.length, \n",
    "                                          j, -np.log10(posterior_pvalues_both_unaff[i][j, 0])))\n",
    "        print(np.hstack((contingency_ihart_aff[i][j], contingency_ssc_aff[i][j])))\n",
    "        print(np.hstack((contingency_ihart_unaff[i][j], contingency_ssc_unaff[i][j])))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([(x.family, x.family_size) for x in all_collections[20][274].matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([[(child_id_to_sex[y], y in x.trans) for y in family_to_individuals[x.family][2:]] for x in all_collections[20][274].matches])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check sex-bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_ihart_m = create_contingency(all_collections, ihart_family_sizes, lambda child_id, is_mat, dataset: child_id_to_sex[child_id]=='1' and dataset == 'iHART')\n",
    "contingency_ihart_f = create_contingency(all_collections, ihart_family_sizes, lambda child_id, is_mat, dataset: child_id_to_sex[child_id]=='2' and dataset == 'iHART')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transrates_ihart_m = [btr.calculate_transmission_rates(contingency_ihart_m[i], ihart_family_sizes) for i in range(len(chroms))]\n",
    "transrates_ihart_f = [btr.calculate_transmission_rates(contingency_ihart_f[i], ihart_family_sizes) for i in range(len(chroms))]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.hstack(transrates_ihart_m)[np.hstack(ind_indices)], np.hstack(transrates_ihart_f)[np.hstack(ind_indices)], alpha=0.1)\n",
    "\n",
    "plt.xlabel('M Trans Rate')\n",
    "plt.ylabel('F Trans Rate')\n",
    "plt.axhline(0.5, color='black', linestyle='--')\n",
    "plt.axvline(0.5, color='black', linestyle='--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posteriors_ihart_m = [btr.calculate_posteriors(contingency_ihart_m[i], ihart_family_sizes) for i in range(len(chroms))]\n",
    "posteriors_ihart_f = [btr.calculate_posteriors(contingency_ihart_f[i], ihart_family_sizes) for i in range(len(chroms))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_ihart_mf = [btr.calculate_posterior_overlap(posteriors_ihart_m[i], posteriors_ihart_f[i]) for i in range(len(chroms))]\n",
    "posterior_pvalues_ihart_m = [btr.calculate_posterior_pvalue(posteriors_ihart_m[i]) for i in range(len(chroms))]\n",
    "posterior_pvalues_ihart_f =  [btr.calculate_posterior_pvalue(posteriors_ihart_f[i]) for i in range(len(chroms))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    " \n",
    "cutoff = -np.log10(0.05/sum([len(contingency_both_aff[i]) for i in range(len(chroms))]))\n",
    "#cutoff = -np.log10(0.05/len(all_collections))\n",
    "\n",
    "titles = ['iHART Overlap', 'iHART M Higher', 'iHART M Lower', 'iHART F Higher', 'iHART F Lower']\n",
    "\n",
    "for i, title in enumerate(titles):\n",
    "    plt.subplot(5, 1, i+1)\n",
    "    plt.axhline(cutoff, linestyle='--', color='black')\n",
    "    plt.ylabel('-log10(pvalue)')\n",
    "    plt.xlabel('Position')\n",
    "    plt.xticks(chrom_offsets[:-1], chroms)\n",
    "    plt.title(title)\n",
    "\n",
    "for i, chrom in enumerate(chroms):\n",
    "    positions = chrom_offsets[i] + np.array([c.deletion.start_pos for c in all_collections[i]])\n",
    "    plt.subplot(5, 1, 1)\n",
    "    plt.plot(positions, -np.log10(overlap_ihart_mf[i]), alpha=0.5)\n",
    "    plt.subplot(5, 1, 2)\n",
    "    plt.plot(positions, -np.log10(posterior_pvalues_ihart_m[i][:, 0]), alpha=0.5)\n",
    "    plt.subplot(5, 1, 3)\n",
    "    plt.plot(positions, -np.log10(posterior_pvalues_ihart_m[i][:, 1]), alpha=0.5)\n",
    "    plt.subplot(5, 1, 4)\n",
    "    plt.plot(positions, -np.log10(posterior_pvalues_ihart_f[i][:, 0]), alpha=0.5)\n",
    "    plt.subplot(5, 1, 5)\n",
    "    plt.plot(positions, -np.log10(posterior_pvalues_ihart_f[i][:, 1]), alpha=0.5)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chrom in enumerate(chroms):\n",
    "    print(chrom)\n",
    "    for j in np.where(-np.log10(posterior_pvalues_ihart_aff[i][:, 0]) > cutoff)[0]:\n",
    "        d = all_collections[i][j].deletion\n",
    "        print('%s%s:%d-%d %d %d %0.2f' % ('*' if ind_indices[i][j] else '', \n",
    "                                       d.chrom, d.opt_start_pos, d.opt_end_pos, d.length, \n",
    "                                          j, -np.log10(posterior_pvalues_both_unaff[i][j, 0])))\n",
    "        print(np.hstack((contingency_ihart_aff[i][j], contingency_ssc_aff[i][j])))\n",
    "        print(np.hstack((contingency_ihart_unaff[i][j], contingency_ssc_unaff[i][j])))\n",
    "    \n",
    "        print(posterior_pvalues_ihart_m[i][j, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
